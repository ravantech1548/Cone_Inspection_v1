# =============================================================================
# Inference Service - Environment Configuration Template
# =============================================================================
# Copy this file to .env and update with your actual values
# =============================================================================

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Path to YOLO model file (relative to inference-service directory)
MODEL_PATH=./models/best.pt

# Directory containing reference images for comparison
REFERENCE_IMAGES_DIR=./reference_images

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
# Port for inference service
PORT=5000

# Host binding
# 0.0.0.0 = accessible from network
# 127.0.0.1 = localhost only
HOST=0.0.0.0

# =============================================================================
# INFERENCE SETTINGS
# =============================================================================
# Default confidence threshold for predictions (0.0 to 1.0)
# This is used when no threshold is specified in the request
# Lower values = more detections but less confident
# Higher values = fewer detections but more confident
DEFAULT_CONFIDENCE_THRESHOLD=0.3

# Minimum allowed confidence threshold
MIN_CONFIDENCE_THRESHOLD=0.1

# Maximum allowed confidence threshold
MAX_CONFIDENCE_THRESHOLD=1.0

# =============================================================================
# HTTPS/TLS CONFIGURATION
# =============================================================================
# Enable HTTPS (true/false)
USE_HTTPS=true

# Path to SSL certificate files (relative to inference-service directory)
TLS_CERT_PATH=../certs/inference-cert.pem
TLS_KEY_PATH=../certs/inference-key.pem

# =============================================================================
# LOGGING CONFIGURATION (Optional)
# =============================================================================
# Log level for inference operations
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# =============================================================================
# PERFORMANCE TUNING (Optional)
# =============================================================================
# Number of worker threads for inference (default: auto)
# INFERENCE_WORKERS=4

# Enable GPU acceleration if available (true/false)
# USE_GPU=false

# Batch size for processing multiple images
# BATCH_SIZE=1

# =============================================================================
# NOTES
# =============================================================================
# 1. Ensure MODEL_PATH points to a valid YOLOv8 classification model
# 2. For intranet access, set HOST=0.0.0.0
# 3. Adjust DEFAULT_CONFIDENCE_THRESHOLD based on your model's performance
# 4. Generate SSL certificates using: ../generate-ssl-certs.ps1 (or .sh)
# 5. Restart the inference service after changing this file
# =============================================================================
